---
title: "Lab4"
output: html_document
date: "2022-12-21"
team: "Peniaha Nazarii, Burak Vasyl, Shamanskyi Kyrylo"
---

All team members did each task together.

```{r}
require(BSDA)
library(BSDA)
require(EnvStats)   
library(EnvStats)
```

## Generating data

```{r}
n <- 18

generate <- function(k){
  a_k <- k*log(k^2*n + pi)
  a_k <- a_k - floor(a_k)
  return (qnorm(a_k))
  
}
X <- generate(seq(1, 100, 1))
Y <- generate(seq(101, 150, 1))
hist(X)
```

## Problem 1

$H_0 : \mu_1 =\mu_2$ vs $H_1 : \mu_1 {\ne} \mu_2$ $\sigma^2_1 = \sigma^2_2 = 1$

As our variance is known, the best test to use in this case is z-test. Therefore, the GLRT suggest to reject $H_0$ when $|z(x,y)| \ge z_{1-\alpha/2}$

```{r}
z.test(x = X, y = Y, alternative = "two.sided", sigma.x = 1, sigma.y = 1);
```

Our p-value = 0.8678, that means that probability to get first type error is large. So we shouldn\`t reject $H_0$

## Problem 2

$H_0 : \sigma_1 ^ 2 = \sigma_2 ^ 2$ vs $H_1: \sigma_1 ^ 2 \ne \sigma_2 ^ 2$; $\mu_0$ and $\mu_1$ are unknown.

Here we will use f-test. Because one of the mains purposes of f-test is to compare two variances.

Rejection region $C_{\alpha} = \{x \in R^n, y \in R^m | f(x, y) \ge f_\alpha \}$

```{r}
var.test(X, Y, alternative = "g")
```

p-value (0.3909) is ≥ $\alpha$ (significance level 0.05), which means we can\`t reject the null hypothesis.

## Problem 3

Kolmogorov's Goodness-of-fit test. Assume $X_1$,...,$X_n$ are i.i.d.r.v from unknown distribution F. For a given hypothetical distribution $F_0$ we want to test

$H_0$ : $F = F_0$ vs $H_1$ : $F \neq F_0$

By LLN, the sample c.d.f $\hat{F_x}$ of $x$ is close to F thus under $H_0$ the statistics

$d = \sup_{t \in R}|\hat{F_x}(t) - F_0(t)|$

should take small values. Under $H_0$, the distribution of d is independent of $F_0$ and is called the Kolmogorov distribution $D_n$ Under $H_1$, d assumes larger values. this is the Kolmogorov's goodness-of-fit test with rejection region

$C_\alpha = \{x \in R^n | d \ge d_{1-\alpha}^{(n)}\}$

(a)$H_0$ are normally distributed vs $H_1$ are not normally distributed.

```{r}
mu1 = mean(X)
mu2 = mean(Y)
ks.test(X, "pnorm", mean = mu1, sd = sd(X))
```

Our p-value = 0.9529, so our first type error is large enough (more than 0.05), and we must accept $H_0$ hypothesis

(b) $H_0$ are exponentially distributed with λ = 1 vs $H_1$ are not exponentially distributed with λ = 1.

```{r}
ks.test(abs(X), "pexp", rate = 1)
```

Our p-value = 0.1626, so our first type error is large enough (more than 0.05), and we must accept $H_0$ hypothesis

(c) $H_0:$ $\{x_k\}_{k=1}^{100}$ and $\{x_l\}_{l=1}^{50}$ have the same distributions vs $H_1$ $\{x_k\}_{k=1}^{100}$ and $\{x_l\}_{l=1}^{50}$ haven\`t the same distributions.

```{r}
ks.test(X,Y)
```

Our p-value = 0.9448, so our first type error is large enough (more than 0.05), and we must accept $H_0$ hypothesis
